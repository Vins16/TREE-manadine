{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización y Modelo de NLP\n",
    "\n",
    "En este notebook utilizaremos el conjunto que hemos inspeccionado y adecuado para crear unas sencillas representaciones de los datos  y poder realizar un sencillo modelo que nos ayude a analizar los sentimientos descritos en las diferentes reseñas. Así, el siguiente script está dividido en los siguientes bloques:\n",
    "\n",
    "- **BLOQUE A**: carga de datos inspeccionados.\n",
    "- **BLOQUE B**: visualización. \n",
    "- **BLOQUE C**: preprocesamiento del texto.\n",
    "- **BLOQUE D**: partición del conjunto de datos en train y test.\n",
    "- **BLOQUE E**: vectorización del texto.\n",
    "- **BLOQUE F**: entrenamiento de distintos modelos.\n",
    "- **BLOQUE G**: inferencia sobre los datos de test.\n",
    "- **BLOQUE H**: exportación del mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE A: Carga de datos\n",
    "Antes de comenzar, cargaremos los datos que han sido adecuados en nuestra fase anterior de limpieza y preprocesamiento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos ya adecuados\n",
    "df = pd.read_csv(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Que dimensiones tiene el conjunto de datos?\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las primeras observaciones del conjunto\n",
    "df.???()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE B: Visualización\n",
    "\n",
    "En este bloque utilizaremos las librerias [matplotlib](https://matplotlib.org/) y [seaborn](https://seaborn.pydata.org/) para crear unas sencillas representaciones de los datos a modo general y descriptivo, mientras que  nos ayudaremos de la librería [wordcloud](https://amueller.github.io/word_cloud/) para poder crear visualizaciones acerca de los textos que vamos a analizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de barras para la variable sentiment\n",
    "sns.countplot(x='???', palette='viridis', data=???)\n",
    "plt.???('Distribución de la variable sentimiento')\n",
    "plt.???('Frecuencia')\n",
    "plt.???('Sentimiento')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico 'pie' con porcentajes para la variable objetivo sentiment\n",
    "plt.???(df['???'].value_counts(), autopct=\"%.2f%%\", labels=['Reseñas Positivas', 'Reseñas negativas'])\n",
    "plt.???()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma de la distribución de las logitudes de las reselas.\n",
    "# Utilizamos los histogramas proporcionados por el propio dataframe.\n",
    "df['???'].hist()\n",
    "plt.???('Histograma de Longitudes')\n",
    "plt.???('Longitud')\n",
    "plt.???('Frecuencia')\n",
    "plt.???()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de la Longitud por cada tipo de sentimiento\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.boxplot(x='???', y='???', data=df, palette='Set2')\n",
    "\n",
    "plt.title('Boxplot de Longitud por Polaridad')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Longitud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear un gráfico de las palabras más comunes en las reseñas de cada tipo de sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos el conjunto de datos para quedarnos para quedarnos solo con reseñas positivas\n",
    "positivedata = df.???[df['sentiment']==???, 'text']\n",
    "\n",
    "# Hacemos lo mismo esta vez con los reseñas negativas\n",
    "negdata = df.???[df['sentiment']==???, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para poder realizar el gráfico\n",
    "def wordcloud_draw(data, color, title):\n",
    "    words = ' '.join(data)\n",
    "    wordcloud = WordCloud(stopwords=stopwords.words('english'),\n",
    "                          background_color=color,\n",
    "                          width=2500,height=2000).generate(words)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos los dos gráficos en una sola visualización\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.subplot(1,2,1)\n",
    "wordcloud_draw(????,'white','Palabras Positivas más comunes')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "wordcloud_draw(???, 'grey','Palabras Negativas más comunes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE C: Preprocesamiento del texto\n",
    "\n",
    "El preprocesamiento del texto es una fase importante dentro del Procesamiento del Lenguaje Natural (NLP). El objetivo de esta fase es la de transformar el texto en crudo, de manera que sea más fácilmente consumible por los algoritmos y modelos de Machine Learning (ML) y Deep Learning (DL) a aplicar.\n",
    "\n",
    "Esta fase consta de diferentes pasos y no son siempre los mismos. En este caso, preprocesaremos los teewts de la siguiente manera:\n",
    "\n",
    "1. **Lower Casing**: Transformar palabras de mayúsculas a minúsculas.\n",
    "\n",
    "2. **Eliminar Non-Alphabets**: Reemplazar todos los caracteres excepto alphabets por un espacio.\n",
    "\n",
    "3. **Eliminar letras consecutivas**: 3 o más letras consecutivas son reemplazadas por 2 letras (ejemplo: \"Heyyyy\" por \"Heyy\").\n",
    "\n",
    "4. **Tokenizacíon**:  proceso de dividir un texto en unidades más pequeñas llamadas tokens (palabras).\n",
    "\n",
    "5. **Eliminar Stopwords**: Las Stopwords son aquellas palabras en ingés que no tienen un significado específico por si solas, por lo que pueden ser ignoradas sin sacrificar el significado de la oración (ejemplos: \"the\", \"a\").\n",
    "\n",
    "6. **Eliminar palabras cortas**: Palabras con menos de 2 letras son eliminadas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para preprocesar el texto en crudo\n",
    "def preprocess(text):    \n",
    "\n",
    "    # Definir patrones para reemplazar/eliminar.\n",
    "    alphaPattern      = \"[^a-zA-Z]\"\n",
    "    sequencePattern   = r\"(.)\\1\\1\\1*\"\n",
    "    seqReplacePattern = r\"\\1\\1\"    \n",
    "\n",
    "    \n",
    "    # Crear lista de stopwords\n",
    "    en_stop =  set(stopwords.words('english')) - {'not','no'}  # set(['a', 'an', 'the', 'in', 'does', 'do'])\n",
    "\n",
    "    # Lower Casing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Reemplazar non-alphabets.\n",
    "    text = re.sub(alphaPattern, \" \", text)\n",
    "\n",
    "     # Reemplazar letras consecutivas.\n",
    "    text = re.sub(sequencePattern, seqReplacePattern, text)\n",
    "    \n",
    "    # Tokenizar texto\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Eliminar stopwords\n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    \n",
    "    # Eliminar stringas con menos de dos elementos\n",
    "    tokens = [word for word in tokens if len(word)>2]\n",
    "    \n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la función a cada una de las reseñas\n",
    "df['preprocess_text'] = df[???].apply(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados del preprocesamiento: un ejemplo\n",
    "print('Texto en crudo:', df.loc[1, ???])\n",
    "print('Texto preprocesado:', df.loc[1, ???])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE D: Partición del conjunto de datos en train y test (80,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['preprocess_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(???, ???, test_size=???, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información acerca de los conjuntos\n",
    "print('Tamaño del conjunto de entrenamiento:', ???(X_train))\n",
    "print('Tamaño del conjunto de test:', ???(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencias relativas de 'Sentiment' en el conjunto de intrenamiento\n",
    "round(???.value_counts(normalize=True), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencias relativas de 'Sentiment' en el conjunto de test\n",
    "round(???.value_counts(normalize=True), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE E: Vectorización del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de dar el texto en input a un modelo es necesario vectorizarlo: convertir las palabras en números.\n",
    "\n",
    "La conversión del texto en una representación númerica es uno de los pasos más importantes dentro de cualquier *pipeline* de NLP. Esta conversión resulta esencial para que las \"máquinas\" puedan comprender y decodificar patrones dentro de cualquier lenguaje.\n",
    "\n",
    "Se trata de un proceso iterativo y que puede ser realizado mediante múltiples maneras o técnicas, abarcando desde las representaciones más sencillas (por ejemlo, One hot encoding) hasta otras más \"inteligentes\", que logran tener en cuenta las similitudes y diferencias entre ellas al basar su aprendizaje en redes neuronales (Word embeddings).\n",
    "\n",
    "En este caso vamos a utilizar la técnica TF-IDF (Term Frequency-Inverse Document Frequency). A continuación, se describen los conceptos clave:\n",
    "\n",
    "1. Term Frequency (TF):\n",
    "Mide la frecuencia de un término específico en un documento.\n",
    "Se calcula dividiendo el número de veces que un término aparece en un documento entre el número total de términos en el documento.\n",
    "Cuanto más frecuente es un término en un documento, mayor es su valor de TF.\n",
    "\n",
    "2. Inverse Document Frequency (IDF):\n",
    "Mide la importancia de un término en el conjunto de documentos.\n",
    "Se calcula tomando el logaritmo del inverso de la proporción de documentos que contienen el término.\n",
    "Términos que aparecen en muchos documentos tendrán un IDF más bajo, ya que se consideran menos informativos.\n",
    "3. TF-IDF:\n",
    "Combina TF y IDF para asignar un peso a cada término en cada documento. \\\n",
    "**TF-IDF = TF * IDF** \\\n",
    "Los términos que son frecuentes en un documento pero raros en el conjunto de documentos tendrán un alto valor de TF-IDF, lo que indica su importancia relativa en ese documento específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorización del texto\n",
    "vectorizer = TfidfVectorizer() \n",
    "\n",
    "# fit_transform() determina qué palabras existen en el conjunto de datos y asigna un índice a cada una de ellas.\n",
    "X_train_vec = vectorizer.???([\" \".join(tokens) for tokens in ???])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformar nuevos datos en función del vocabulario aprendido anteriormente\n",
    "X_test_vec = vectorizer.???([\" \".join(tokens) for tokens in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver con más detalle el objecto generado con TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tipo de objeto\n",
    "???(X_train_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Matriz dispersa (sparse matrix) en el formato CSR (Compressed Sparse Row). Una matriz dispersa es una estructura de datos que se utiliza para almacenar matrices que tienen una gran cantidad de elementos cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener dimensiones\n",
    "num_documentos, num_terminos = X_train_vec.???\n",
    "\n",
    "print(f\"Número de Documentos: {num_documentos}\")\n",
    "print(f\"Número de Términos: {num_terminos}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogemos 10 palabras al azar\n",
    "random.???(list(vectorizer.get_feature_names_out()), ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver el valor TF-IDF asignados a algunas palabras en el primer documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a ver que contiene el primer documento\n",
    "X_train.???[???]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminos = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Obtener el primer documento como vector TF-IDF\n",
    "vector_tfidf_primer_documento = X_train_vec[0]\n",
    "\n",
    "# Crear un DataFrame para visualizar el resultado\n",
    "df = pd.???(vector_tfidf_primer_documento.toarray(), columns=terminos)\n",
    "\n",
    "# valor TF-IDF asignado a la palabra 'saw'\n",
    "df[???]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valor TF-IDF asignado a la palabra 'bad' (no presente en el documento)\n",
    "df['???']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE F: Entrenamiento de distinto modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión logistica\n",
    "\n",
    "La regresión logística es un método de clasificación que modela la probabilidad de eventos binarios. Utilizando la función sigmoide, asigna valores entre 0 y 1, facilitando la predicción de categorías, como positivo o negativos, en aplicaciones prácticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo\n",
    "log_model = ???()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento o ajuste del modelo con los datos de entrenamiento\n",
    "log_model.???(???, ???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre los datos de entrenamiento\n",
    "y_pred_train = log_model.???(X_train_vec)\n",
    "\n",
    "# Mostramos el \"classification report\"\n",
    "print('Resultados conjunto de entrenamiento:\\n')\n",
    "print(classification_report(???, ????))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es Boosting?\n",
    "\n",
    "Boosting es un meta-algoritmo de aprendizaje automático que reduce el sesgo y la varianza en un contexto de aprendizaje supervisado. Consiste en combinar los resultados de varios clasificadores débiles para obtener un clasificador robusto. Cuando se añaden estos clasificadores débiles, se hace de modo que éstos tengan diferente peso en función de la exactitud de sus predicciones. Tras añadir un clasificador débil, los datos cambian su estructura de pesos: los casos mal clasificados ganan peso y los que son clasificados correctamente pierden peso.\n",
    "\n",
    "Gradient Boosting (GB) o Potenciación del gradiente consiste en plantear el problema como una optimización numérica en el que el objetivo es minimizar una función de coste añadiendo clasificadores débiles mediante el descenso del gradiente. Involucra tres elementos:\n",
    "\n",
    "La función de coste a optimizar: depende del tipo de problema a resolver.\n",
    "Un clasificador débil para hacer las predicciones: por lo general se usan árboles de decisión.\n",
    "Un modelo que añade (ensambla) los clasificadores débiles para minimizar la función de coste: se usa el descenso del gradiente para minimizar el coste al añadir árboles.\n",
    "Los hiperparámetros más importantes que intervienen en este algoritmo (aunque no todos) son:\n",
    "\n",
    "learning_rate: determina el impacto de cada árbol en la salida final. Se parte de una estimación inicial que se va actualizando con la salida de cada árbol. Es el parámetro que controla la magnitud de las actualizaciones.\n",
    "n_estimators: número de clasificadores débiles a utilizar.\n",
    "Como en este caso utilizaremos árboles de decisión como clasificadores débiles a ensamblar, también debemos tener en cuenta los hiperparámetros que afectan a esta clase de modelos. En este caso:\n",
    "\n",
    "max_depth: profundidad máxima del árbol.\n",
    "\n",
    "Más información sobre el modelo que se utiliza en este ejemplo y de sus parámetros [aquí](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo introduciendo los valores de los parámetros:\n",
    "gb_clf = ???(n_estimators=150, learning_rate=0.2, max_depth=3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento o ajuste del modelo con los datos de entrenamiento\n",
    "gb_clf.???(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos sobre los datos de entrenamiento\n",
    "pred_train = gb_clf.???(X_train_vec)\n",
    "\n",
    "# Mostramos el \"classification report\"\n",
    "print('Resultados conjunto de entrenamiento:\\n')\n",
    "print(classification_report(???, ???))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE G: Inferencia sobre los datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferencia con la regressión logistica\n",
    "y_pred = ???(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo de regresión logisitca sobre el conjunto de test\n",
    "\n",
    "# Mostramos el \"classification report\" y \"accuracy\"\n",
    "accuracy = ???(y_test, y_pred)\n",
    "\n",
    "print('Resultados conjunto de entrenamiento:\\n')\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n",
    "print(???(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferencia con el modelo de gradient boosting\n",
    "pred_test = ???(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo de regresión logisitca sobre el conjunto de test\n",
    "\n",
    "# Mostramos el \"classification report\" y \"accuracy\"\n",
    "accuracy = ???(y_test, pred_test)\n",
    "\n",
    "print('Resultados conjunto de entrenamiento:\\n')\n",
    "print(f'Accuracy: {accuracy:.2f}\\n')\n",
    "print(???(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE H: Exportación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/gradient_boosting_model.pkl', 'wb') as file:\n",
    "    pickle.dump(gb_clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el vectorizer en un file file\n",
    "joblib.dump(vectorizer, 'models/tfidf_vectorizer.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
